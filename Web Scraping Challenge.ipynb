{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Delish Recipes\n",
    "\n",
    "My goals are to write a scraper that will:\n",
    "\n",
    "1) Return specific pieces of information (rather than just downloading a whole page)  \n",
    "\n",
    "2) Iterate over multiple pages/queries  \n",
    "\n",
    "3) Save the data to my computer\n",
    "\n",
    "Let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Importing in each cell because of the kernel restarts.\n",
    "import scrapy\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "class DelishSpider(scrapy.Spider):\n",
    "    # Give it a name\n",
    "    name = 'DES'\n",
    "    \n",
    "    # Establish a URL to start with\n",
    "    start_urls = ['https://www.delish.com/cooking/recipe-ideas']\n",
    "    \n",
    "    # Parse out the responses we get\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # Parse the list of recipes\n",
    "        all_recipes = response.xpath('//div[@class=\"full-item-content\"]')\n",
    "        \n",
    "        # Find the links for the each recipe\n",
    "        for recipe in all_recipes:\n",
    "            rec_url = self.start_urls[0] + recipe.xpath('.//a/@href').extract_first()\n",
    "            \n",
    "            # Yield this URL so we can scrape inside it\n",
    "            yield scrapy.Request(rec_url, callback=self.parse_recipe)\n",
    "    \n",
    "    # This portion scrapes from inside the recipe link\n",
    "    def parse_recipe(self, response):\n",
    "        title = response.xpath('//div/h1/text()').extract_first()\n",
    "        author = response.xpath('//div/a/span[@class=\"byline-name\"]/text()').extract_first()\n",
    "        intro = response.xpath('//div[@class=\"recipe-introduction show-more\"]/p/text()').extract_first()\n",
    "        directions = response.xpath('//div[@class=\"direction-lists\"]').extract_first()\n",
    "        \n",
    "        # sometimes the author is in a different spot though\n",
    "        if author is None:\n",
    "            author = response.xpath('//div/span[@class=\"byline-name\"]/text()').extract_first()\n",
    "        \n",
    "        # depending how long the intro is it might say \"show less\" instead of \"more\"\n",
    "        if intro is None:\n",
    "            intro = response.xpath('//div[@class=\"recipe-introduction show-less\"]/p/text()').extract_first()\n",
    "        \n",
    "            # if intro is still empty, this is probably a slideshow page with no directions\n",
    "            if intro is None:\n",
    "                intro = response.xpath('//div[@class=\"slideshow-desktop-dek\"]/p/span/text()').extract_first()\n",
    "                directions = 'slideshow'\n",
    "        \n",
    "        yield {\n",
    "            'title' : title,\n",
    "            'author' : author,\n",
    "            'introduction' : intro,\n",
    "            'directions' : directions\n",
    "        }\n",
    "        \n",
    "        \n",
    "\n",
    "# Pass the script some scraping etiquette       \n",
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',                                # Store data in JSON format\n",
    "    'FEED_URI': 'delish_data.json',                       # Name our storage file\n",
    "    'LOG_ENABLED': False,                                 # Turn off logging for now\n",
    "    'ROBOTSTXT_OBEY': True,                               # Obey the robots.txt rules\n",
    "    'USER_AGENT': 'GABootcampCrawler (thinkful.com)', # Tell them who you are\n",
    "    'AUTOTHROTTLE_ENABLED': True,                         # Automatically throttle back request rate\n",
    "    'HTTPCACHE_ENABLED': True                             # Keep websites we already visited cached\n",
    "})\n",
    "\n",
    "# Start the crawler with our spider.\n",
    "process.crawl(DelishSpider)\n",
    "process.start()\n",
    "print('Success!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
